# -*- coding: utf-8 -*-
"""ASR_piada_mp3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dRJsjhvGQpvt0EIR2EqsS6uYGZNIepIW
"""

### **Pipeline Completo - Aula 01: Reconhecimento Automático de Fala (ASR)**

# **Passo 1: Configuração do Ambiente e Upload do Áudio**
from google.colab import files
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
from ipywidgets import widgets

# Botão para enviar áudio
print("Envie o arquivo de áudio para análise:")
uploaded = files.upload()
audio_path = list(uploaded.keys())[0]  # Obtém o nome do arquivo enviado

# **Passo 2: Pré-processamento do Áudio**
def preprocess_audio(audio_path):
    """
    Carrega o áudio, remove ruído simples (subtração de média) e normaliza o sinal.
    Retorna o sinal pré-processado e a taxa de amostragem.

    Argumentos:
        audio_path (str): Caminho para o arquivo de áudio a ser processado.

    Retorna:
        tuple: Um sinal de áudio pré-processado e a taxa de amostragem correspondente.
    """
    # Carregar o áudio
    # `librosa.load` retorna o sinal de áudio (y) e a taxa de amostragem (sr).
    # Aqui, a taxa de amostragem é definida como 16kHz para ser compatível com a maioria dos modelos de ASR.
    print("Carregando o áudio...")
    y, sr = librosa.load(audio_path, sr=16000)  # Taxa de amostragem padrão 16kHz

    # Exibir informações básicas do áudio carregado
    print(f"Duração do áudio: {len(y) / sr:.2f} segundos")
    print(f"Taxa de amostragem: {sr} Hz")

    # Salvar o ruído original (média do sinal)
    ruido = np.mean(y)
    print(f"Ruído detectado (valor médio): {ruido:.4f}")

    # Remoção simples de ruído (subtração da média)
    # Esta etapa reduz o ruído estacionário removendo a média do sinal.
    # Isso é especialmente útil para eliminar desvios constantes que não representam a fala.
    print("Removendo ruído (subtração da média)...")
    y_denoised = y - ruido

    # Normalização
    # A normalização ajusta os valores de amplitude do sinal para ficarem dentro de uma faixa padrão.
    # Isso ajuda a evitar problemas de saturação e garante que regiões de baixa amplitude sejam destacadas.
    print("Normalizando o áudio...")
    y_normalized = librosa.util.normalize(y_denoised)

    # Análise estatística do sinal antes e depois da normalização
    print("Análise do sinal:")
    print(f"Amplitude original: Min={y.min():.4f}, Max={y.max():.4f}, Média={y.mean():.4f}")
    print(f"Amplitude sem ruído: Min={y_denoised.min():.4f}, Max={y_denoised.max():.4f}, Média={y_denoised.mean():.4f}")
    print(f"Amplitude normalizada: Min={y_normalized.min():.4f}, Max={y_normalized.max():.4f}, Média={y_normalized.mean():.4f}")

    # Plotar o áudio original, ruído e pré-processado
    # Visualizações úteis para comparar os efeitos do pré-processamento no sinal.
    print("Plotando o sinal de áudio...")
    plt.figure(figsize=(14, 12))

    # Sinal original
    plt.subplot(3, 1, 1)
    librosa.display.waveshow(y, sr=sr)
    plt.title("Áudio Original")
    plt.xlabel("Tempo (s)")
    plt.ylabel("Amplitude")

    # Ruído detectado
    plt.subplot(3, 1, 2)
    plt.axhline(y=ruido, color="red", linestyle="--", label="Ruído Médio")
    librosa.display.waveshow(y, sr=sr, alpha=0.5)
    plt.title("Ruído Estacionário no Áudio")
    plt.xlabel("Tempo (s)")
    plt.ylabel("Amplitude")
    plt.legend()

    # Sinal pré-processado
    plt.subplot(3, 1, 3)
    librosa.display.waveshow(y_normalized, sr=sr)
    plt.title("Áudio Pré-processado")
    plt.xlabel("Tempo (s)")
    plt.ylabel("Amplitude")

    plt.tight_layout()
    plt.show()

    # Retornar o sinal pré-processado e a taxa de amostragem
    return y_normalized, sr

# Chamando a função para pré-processar o áudio e obter o sinal ajustado e a taxa de amostragem
print("Iniciando o pré-processamento do áudio...")
signal, sample_rate = preprocess_audio(audio_path)

# Verificando a duração final do áudio processado
print(f"Sinal pré-processado carregado com sucesso. Duração: {len(signal) / sample_rate:.2f} segundos")

# **Passo 3: Extração de Características do Áudio**
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

def extract_audio_features(audio_signal, sample_rate):
    """
    Extrai características importantes do sinal de áudio, como MFCCs, Mel Spectrogram e características temporais.

    Argumentos:
        audio_signal (numpy.ndarray): Sinal de áudio pré-processado.
        sample_rate (int): Taxa de amostragem do sinal.

    Retorna:
        dict: Um dicionário contendo as características extraídas (MFCC, Mel Spectrogram, RMS).
    """
    print("Extraindo características do áudio...")

    # **1. MFCC (Mel-Frequency Cepstral Coefficients)**
    # Os MFCCs representam a energia do sinal em diferentes bandas de frequência baseadas na escala Mel.
    # Isso é amplamente utilizado em tarefas como reconhecimento de fala.
    #Por padrão, 12-13 coeficientes são usados na maioria das aplicações de processamento de fala.
    #A taxa de amostragem do áudio (em Hz): indica quantas amostras por segundo o áudio possui (16 kHz, sr=16000)
    mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=13)
    print(f"MFCCs extraídos: {mfccs.shape[1]} frames com {mfccs.shape[0]} coeficientes")

    # **2. Mel Spectrogram**
    # Representação do espectro de frequências na escala Mel (mais próxima da percepção humana).
    # n_fft: Número de pontos para a Transformada Rápida de Fourier (FFT).
    # hop_length: Passo (número de amostras) entre janelas consecutivas.
    # n_mels: Número de bandas na escala Mel.

    mel_spectrogram = librosa.feature.melspectrogram(y=audio_signal, sr=sample_rate, n_fft=2048, hop_length=512, n_mels=128)
    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)  # Converte para dB para melhor visualização

    # **3. RMS (Root Mean Square)**
    # Calcula a energia média em cada frame do áudio.
    rms = librosa.feature.rms(y=audio_signal)
    print(f"RMS extraído: {rms.shape[1]} frames")

      # **Visualizações**
    plt.figure(figsize=(14, 10))

    # MFCCs
    plt.subplot(3, 1, 1)
    librosa.display.specshow(mfccs, x_axis='time', sr=sample_rate, cmap='coolwarm')
    plt.colorbar()
    plt.title("MFCC (Mel-Frequency Cepstral Coefficients)")

    # Mel Spectrogram
    plt.subplot(3, 1, 2)
    librosa.display.specshow(mel_spectrogram_db, x_axis='time', y_axis='mel', sr=sample_rate, cmap='coolwarm')
    plt.colorbar()
    plt.title("Mel Spectrogram (em dB)")

    # RMS
    plt.subplot(3, 1, 3)
    plt.plot(rms[0], label="RMS")
    plt.xlabel("Frames")
    plt.ylabel("Amplitude RMS")
    plt.title("Energia RMS do Sinal")
    plt.legend()

    plt.tight_layout()
    plt.show()

    return {
        "MFCC": mfccs,
        "Mel Spectrogram": mel_spectrogram_db,
        "RMS": rms
    }

# Chamando a função de extração de características
print("Iniciando a extração de características...")
features = extract_audio_features(signal, sample_rate)

# Exibindo algumas características extraídas
print("Características extraídas com sucesso:")
print(f"MFCC (shape): {features['MFCC'].shape}")
print(f"Mel Spectrogram (shape): {features['Mel Spectrogram'].shape}")
print(f"RMS (shape): {features['RMS'].shape}")

'''
# **Passo 4: Reconhecimento de Fala (Speech-to-Text - STT)**


# Importação de bibliotecas necessárias
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, pipeline
import torchaudio
import torch
import numpy as np
import wave

# **Função 1: Reconhecimento de fala com Wav2Vec 2.0**
def speech_to_text_wav2vec(audio_path):
    """
    Usa o modelo Wav2Vec 2.0 para converter áudio em texto transcrito.
    Retorna o texto transcrito.
    """
    print("Iniciando reconhecimento de fala com Wav2Vec 2.0...")

    # Carregar o processador e o modelo Wav2Vec 2.0
    #processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h")
    model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h")

     # Carregar o processador e o modelo ajustado ao português
    processor = Wav2Vec2Processor.from_pretrained("jonatasgrosman/wav2vec2-large-xlsr-53-portuguese")
    model = Wav2Vec2ForCTC.from_pretrained("jonatasgrosman/wav2vec2-large-xlsr-53-portuguese")


    # Carregar o áudio
    waveform, sr = torchaudio.load(audio_path)
    if sr != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)
        waveform = resampler(waveform)

    # Pré-processamento do áudio
    input_values = processor(waveform.squeeze().numpy(), return_tensors="pt", sampling_rate=16000).input_values

    # Obter previsões do modelo
    logits = model(input_values).logits

    # Decodificar os IDs preditos em texto
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.decode(predicted_ids[0])

    print("Texto Transcrito (Wav2Vec 2.0):", transcription)
    return transcription


# Caminho para o arquivo de áudio
# Use o mesmo arquivo processado anteriormente
transcription_wav2vec = speech_to_text_wav2vec(audio_path)


# **Resumo das transcrições**
print("\n--- Resumo das Transcrições ---")
print(f"Wav2Vec 2.0: {transcription_wav2vec}")
'''

# **Passo 4: Reconhecimento de Fala (Speech-to-Text - STT)**


# Importação de bibliotecas necessárias
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, pipeline
import torchaudio
import torch
import numpy as np
import wave


# **Função Whisper: Reconhecimento de fala com Whisper**
def speech_to_text_whisper(audio_path):
    """
    Usa o modelo Whisper para converter áudio em texto transcrito.
    Retorna o texto transcrito.
    """
    print("Iniciando reconhecimento de fala com Whisper...")

    # Carregar o pipeline do modelo Whisper
    transcriber = pipeline("automatic-speech-recognition", model="openai/whisper-large")

    # Realizar a transcrição
    # transcription = transcriber(audio_path)["text"]
    transcription = transcriber(audio_path, return_timestamps=True)["text"]

    print("Texto Transcrito (Whisper):", transcription)
    return transcription

# **Chamando as funções de reconhecimento de fala**
print("Executando transcrições com diferentes modelos...")

# Caminho para o arquivo de áudio
# Use o mesmo arquivo processado anteriormente
transcription_whisper = speech_to_text_whisper(audio_path)

# **Resumo das transcrições**
print("\n--- Resumo das Transcrições ---")
print(f"Whisper: {transcription_whisper}")

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Aplicar K-Means
n_clusters = 3  # Escolha o número de clusters
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters = kmeans.fit_predict(consolidated_matrix)

# Reduzir as dimensões para visualização (PCA)
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(consolidated_matrix)

# Plotar os clusters
plt.figure(figsize=(10, 6))
for cluster in range(n_clusters):
    plt.scatter(reduced_data[clusters == cluster, 0],
                reduced_data[clusters == cluster, 1], label=f"Cluster {cluster}")
plt.title("Clusters dos Frames de Áudio")
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.legend()
plt.show()

## Corrigido pelo Gemini

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import numpy as np

# Assuming 'features' is a dictionary containing MFCCs, Mel Spectrogram, and RMS:
mfccs = features['MFCC']
mel_spectrogram = features['Mel Spectrogram']
rms = features['RMS']

# Reshape MFCCs and Mel Spectrogram to 2D if necessary
mfccs = mfccs.T  # Transpose to have frames as rows
mel_spectrogram = mel_spectrogram.T  # Transpose to have frames as rows

# Reshape RMS to 2D if necessary
rms = rms.T

# Choose features for clustering
# Here, we'll use MFCCs
# Adjust as needed based on the features you want to use
consolidated_matrix = mfccs

# Apply K-Means
n_clusters = 3  # Escolha o número de clusters
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters = kmeans.fit_predict(consolidated_matrix)

# Reduce dimensions for visualization (PCA)
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(consolidated_matrix)

# Plot the clusters
plt.figure(figsize=(10, 6))
for cluster in range(n_clusters):
    plt.scatter(reduced_data[clusters == cluster, 0],
                reduced_data[clusters == cluster, 1], label=f"Cluster {cluster}")
plt.title("Clusters dos Frames de Áudio")
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.legend()
plt.show()

"""###Clusters representam tipos de sons:

Cluster 0 (azul) pode representar frames de silêncio.
Cluster 1 (laranja) pode representar fala mais intensa.
Cluster 2 (verde) pode representar ruído ou outros tipos de sons.
"""

from sklearn.ensemble import IsolationForest

# Aplicar Isolation Forest para detectar outliers
isolation_forest = IsolationForest(contamination=0.05, random_state=42)
outlier_labels = isolation_forest.fit_predict(consolidated_matrix)

# Separar os outliers
outliers = consolidated_matrix[outlier_labels == -1]

print(f"Número de Outliers Detectados: {len(outliers)}")

# Visualizar os outliers em comparação com os dados normais (com PCA)
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(consolidated_matrix)
plt.figure(figsize=(10, 6))
plt.scatter(reduced_data[outlier_labels == 1, 0], reduced_data[outlier_labels == 1, 1], c='blue', label='Normal')
plt.scatter(reduced_data[outlier_labels == -1, 0], reduced_data[outlier_labels == -1, 1], c='red', label='Outliers')
plt.title("Detecção de Outliers na Matriz Consolidada")
plt.legend()
plt.show()

## Corrigido pelo Gemini

from sklearn.ensemble import IsolationForest
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Assuming 'features' is a dictionary containing MFCCs, Mel Spectrogram, and RMS:
# This code block was copied from ipython-input-10-5ee32467089e
# to define the consolidated_matrix variable here
mfccs = features['MFCC']
mel_spectrogram = features['Mel Spectrogram']
rms = features['RMS']

# Reshape MFCCs and Mel Spectrogram to 2D if necessary
mfccs = mfccs.T  # Transpose to have frames as rows
mel_spectrogram = mel_spectrogram.T  # Transpose to have frames as rows

# Reshape RMS to 2D if necessary
rms = rms.T

# Choose features for clustering
# Here, we'll use MFCCs
# Adjust as needed based on the features you want to use
consolidated_matrix = mfccs
# End of copied code


# Aplicar Isolation Forest para detectar outliers
isolation_forest = IsolationForest(contamination=0.05, random_state=42)
outlier_labels = isolation_forest.fit_predict(consolidated_matrix)

# Separar os outliers
outliers = consolidated_matrix[outlier_labels == -1]

print(f"Número de Outliers Detectados: {len(outliers)}")

# Visualizar os outliers em comparação com os dados normais (com PCA)
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(consolidated_matrix)
plt.figure(figsize=(10, 6))
plt.scatter(reduced_data[outlier_labels == 1, 0], reduced_data[outlier_labels == 1, 1], c='blue', label='Normal')
plt.scatter(reduced_data[outlier_labels == -1, 0], reduced_data[outlier_labels == -1, 1], c='red', label='Outliers')
plt.title("Detecção de Outliers na Matriz Consolidada")
plt.legend()
plt.show()

"""###Os outliers podem indicar:
Ruído ou distorção no áudio (ex.: ruído de fundo intenso, cliques ou interferências).
Eventos raros ou incomuns, como trechos com palavras diferentes ou mudanças bruscas no áudio.
Erros na extração de características, como valores extremos nas dimensões de MFCC ou energia RMS.
"""